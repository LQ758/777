import numpy as np
import torch
import librosa
from typing import List, Dict, Tuple, Optional
import re
from dataclasses import dataclass
import traceback

@dataclass
class PhonemeScore:
    """éŸ³ç´ è¯„åˆ†ç»“æœ"""
    phoneme: str          # éŸ³ç´ ç¬¦å·
    start_time: float     # å¼€å§‹æ—¶é—´(ç§’)
    end_time: float       # ç»“æŸæ—¶é—´(ç§’)
    score: float          # è¯„åˆ†(0-100)
    confidence: float     # ç½®ä¿¡åº¦(0-1)
    quality: str          # è´¨é‡ç­‰çº§(excellent/good/fair/poor)
    issues: List[str]     # å‘éŸ³é—®é¢˜åˆ—è¡¨

@dataclass
class DetailedPronunciationResult:
    """è¯¦ç»†å‘éŸ³è¯„åˆ†ç»“æœ"""
    overall_score: float                    # æ€»åˆ†(0-100)
    phoneme_scores: List[PhonemeScore]      # éŸ³ç´ çº§è¯„åˆ†
    word_scores: List[Dict]                 # å•è¯çº§è¯„åˆ†
    pronunciation_issues: List[str]         # å‘éŸ³é—®é¢˜æ€»ç»“
    improvement_suggestions: List[str]      # æ”¹è¿›å»ºè®®
    duration_analysis: Dict                 # æ—¶é•¿åˆ†æ
    pitch_analysis: Dict                    # è¯­è°ƒåˆ†æ

class PhonemeScorer:
    """éŸ³ç´ çº§å‘éŸ³è¯„åˆ†å™¨"""
    
    def __init__(self):
        self.phoneme_map = self._load_phoneme_map()
        self.duration_thresholds = self._load_duration_thresholds()
        
    def _load_phoneme_map(self) -> Dict[str, str]:
        """åŠ è½½éŸ³ç´ æ˜ å°„è¡¨ï¼ˆæ–‡æœ¬åˆ°IPAéŸ³ç´ ï¼‰"""
        # ç®€åŒ–çš„éŸ³ç´ æ˜ å°„ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å®Œæ•´çš„æ˜ å°„
        return {
            # å…ƒéŸ³
            'a': 'Ã¦', 'e': 'e', 'i': 'Éª', 'o': 'É’', 'u': 'ÊŠ',
            'ah': 'ÊŒ', 'ay': 'aÉª', 'aw': 'aÊŠ', 'oy': 'É”Éª',
            'ee': 'iË', 'oo': 'uË', 'er': 'ÉœË', 'ar': 'É‘Ë',
            
            # è¾…éŸ³
            'p': 'p', 'b': 'b', 't': 't', 'd': 'd', 'k': 'k', 'g': 'g',
            'f': 'f', 'v': 'v', 'th': 'Î¸', 'dh': 'Ã°', 's': 's', 'z': 'z',
            'sh': 'Êƒ', 'zh': 'Ê’', 'ch': 'tÊƒ', 'j': 'dÊ’',
            'm': 'm', 'n': 'n', 'ng': 'Å‹', 'l': 'l', 'r': 'r',
            'w': 'w', 'y': 'j', 'h': 'h'
        }
    
    def _load_duration_thresholds(self) -> Dict[str, Tuple[float, float]]:
        """åŠ è½½éŸ³ç´ æ—¶é•¿é˜ˆå€¼"""
        return {
            # æ ¼å¼: éŸ³ç´ : (æœ€å°æ—¶é•¿, æœ€å¤§æ—¶é•¿) å•ä½ï¼šç§’
            'Ã¦': (0.08, 0.15), 'Éª': (0.06, 0.12), 'ÊŠ': (0.06, 0.12),
            'iË': (0.10, 0.20), 'uË': (0.10, 0.20), 'ÉœË': (0.12, 0.25),
            'p': (0.02, 0.08), 'b': (0.03, 0.10), 't': (0.02, 0.08),
            'd': (0.03, 0.10), 'k': (0.02, 0.08), 'g': (0.03, 0.10),
            'f': (0.08, 0.15), 'v': (0.06, 0.12), 's': (0.08, 0.18),
            'z': (0.06, 0.15), 'Êƒ': (0.08, 0.16), 'Ê’': (0.06, 0.14),
        }
    
    def text_to_phonemes(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºéŸ³ç´ åºåˆ—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦ä½¿ç”¨ä¸“ä¸šçš„phonemizer
        text = text.lower().strip()
        words = text.split()
        phonemes = []
        
        # åŸºç¡€çš„éŸ³ç´ è½¬æ¢è§„åˆ™
        phoneme_rules = {
            'the': ['Ã°', 'É™'],
            'and': ['Ã¦', 'n', 'd'],
            'have': ['h', 'Ã¦', 'v'],
            'that': ['Ã°', 'Ã¦', 't'],
            'for': ['f', 'É”Ë'],
            'are': ['É‘Ë'],
            'with': ['w', 'Éª', 'Î¸'],
            'his': ['h', 'Éª', 'z'],
            'they': ['Ã°', 'eÉª'],
            'this': ['Ã°', 'Éª', 's'],
            'from': ['f', 'r', 'É’', 'm'],
            'she': ['Êƒ', 'iË'],
            'her': ['h', 'ÉœË'],
            'been': ['b', 'iË', 'n'],
            'than': ['Ã°', 'Ã¦', 'n'],
            'its': ['Éª', 't', 's'],
            'now': ['n', 'aÊŠ'],
            'more': ['m', 'É”Ë'],
            'very': ['v', 'e', 'r', 'Éª'],
            'what': ['w', 'É’', 't'],
            'know': ['n', 'É™ÊŠ'],
            'just': ['dÊ’', 'ÊŒ', 's', 't'],
            'first': ['f', 'ÉœË', 's', 't'],
            'time': ['t', 'aÉª', 'm'],
            'people': ['p', 'iË', 'p', 'É™', 'l'],
        }
        
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word)
            if word_clean in phoneme_rules:
                phonemes.extend(phoneme_rules[word_clean])
            else:
                # ç®€å•çš„å­—æ¯åˆ°éŸ³ç´ æ˜ å°„
                for char in word_clean:
                    if char in self.phoneme_map:
                        phonemes.append(self.phoneme_map[char])
                    else:
                        phonemes.append(char)  # ä¿ç•™æœªçŸ¥å­—ç¬¦
        
        return phonemes
    
    def extract_acoustic_features(self, audio_data: np.ndarray, sr: int = 16000) -> Dict:
        """æå–å£°å­¦ç‰¹å¾"""
        try:
            features = {}
            
            # åŸºé¢‘(F0)æå–
            f0 = librosa.yin(audio_data, fmin=80, fmax=400, sr=sr)
            features['f0'] = f0
            features['f0_mean'] = np.nanmean(f0[f0 > 0]) if np.any(f0 > 0) else 0
            features['f0_std'] = np.nanstd(f0[f0 > 0]) if np.any(f0 > 0) else 0
            
            # MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)
            features['mfcc'] = mfcc
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            
            # é¢‘è°±è´¨å¿ƒå’Œå¸¦å®½
            spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sr)[0]
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr)[0]
            features['spectral_centroid_mean'] = np.mean(spectral_centroids)
            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)
            
            # èƒ½é‡ç‰¹å¾
            rms = librosa.feature.rms(y=audio_data)[0]
            features['energy_mean'] = np.mean(rms)
            features['energy_std'] = np.std(rms)
            
            # é›¶äº¤å‰ç‡
            zcr = librosa.feature.zero_crossing_rate(audio_data)[0]
            features['zcr_mean'] = np.mean(zcr)
            
            return features
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return {}
    
    def force_align_ctc(self, audio_data: np.ndarray, phoneme_sequence: List[str], 
                       wav2vec2_model, processor, sr: int = 16000) -> List[Tuple[str, float, float]]:
        """ä½¿ç”¨Wav2Vec2 CTCè¿›è¡Œå¼ºåˆ¶å¯¹é½"""
        try:
            # é¢„å¤„ç†éŸ³é¢‘
            inputs = processor(audio_data, sampling_rate=sr, return_tensors="pt", padding=True)
            
            # è·å–CTCè¾“å‡º
            with torch.no_grad():
                logits = wav2vec2_model(**inputs).logits
            
            # è·å–é¢„æµ‹åºåˆ—
            predicted_ids = torch.argmax(logits, dim=-1)
            
            # ç®€åŒ–çš„å¯¹é½ï¼šå‡åŒ€åˆ†å‰²æ—¶é—´
            # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„CTCå¯¹é½ç®—æ³•
            total_duration = len(audio_data) / sr
            phoneme_count = len(phoneme_sequence)
            
            if phoneme_count == 0:
                return []
            
            avg_duration = total_duration / phoneme_count
            alignments = []
            
            for i, phoneme in enumerate(phoneme_sequence):
                start_time = i * avg_duration
                end_time = (i + 1) * avg_duration
                alignments.append((phoneme, start_time, end_time))
            
            return alignments
            
        except Exception as e:
            print(f"å¼ºåˆ¶å¯¹é½å¤±è´¥: {e}")
            # è¿”å›å‡åŒ€åˆ†å‰²çš„å¯¹é½ç»“æœ
            total_duration = len(audio_data) / sr
            phoneme_count = len(phoneme_sequence)
            
            if phoneme_count == 0:
                return []
            
            avg_duration = total_duration / phoneme_count
            alignments = []
            
            for i, phoneme in enumerate(phoneme_sequence):
                start_time = i * avg_duration
                end_time = (i + 1) * avg_duration
                alignments.append((phoneme, start_time, end_time))
            
            return alignments
    
    def score_phoneme_quality(self, phoneme: str, features: Dict, duration: float) -> Tuple[float, List[str]]:
        """è¯„ä¼°å•ä¸ªéŸ³ç´ çš„å‘éŸ³è´¨é‡ï¼ˆæ›´åŠ ä¸¥æ ¼çš„è¯„åˆ†æ ‡å‡†ï¼‰"""
        score = 80.0  # é™ä½åŸºç¡€åˆ†æ•°ï¼Œä½¿è¯„åˆ†æ›´åŠ ä¸¥æ ¼
        issues = []
        
        # æ›´ä¸¥æ ¼çš„æ—¶é•¿è¯„ä¼°
        if phoneme in self.duration_thresholds:
            min_dur, max_dur = self.duration_thresholds[phoneme]
            if duration < min_dur * 0.7:  # æ›´ä¸¥æ ¼çš„ä¸‹é™
                score -= 30
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³è¿‡çŸ­ï¼Œéœ€è¦æ›´å……åˆ†çš„å‘å£°")
            elif duration < min_dur:
                score -= 20
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³ç•¥çŸ­")
            elif duration > max_dur * 1.5:  # æ›´ä¸¥æ ¼çš„ä¸Šé™
                score -= 25
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³è¿‡é•¿ï¼Œæ³¨æ„æ§åˆ¶èŠ‚å¥")
            elif duration > max_dur:
                score -= 15
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³ç•¥é•¿")
        
        # åŸºäºMFCCçš„è´¨é‡è¯„ä¼°ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if 'mfcc_mean' in features and len(features['mfcc_mean']) > 0:
            mfcc_stability = np.std(features['mfcc_mean'])
            if mfcc_stability > 30:  # é™ä½é˜ˆå€¼ï¼Œæ›´ä¸¥æ ¼
                score -= 20
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³ä¸ç¨³å®šï¼Œå¯èƒ½å­˜åœ¨ç´§å¼ æˆ–ä¸ç¡®å®š")
            elif mfcc_stability > 20:
                score -= 10
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³ç¨æ˜¾ä¸ç¨³å®š")
        
        # æ›´ä¸¥æ ¼çš„èƒ½é‡è¯„ä¼°
        if 'energy_mean' in features:
            if features['energy_mean'] < 0.005:  # æé«˜é˜ˆå€¼
                score -= 25
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³èƒ½é‡ä¸è¶³ï¼Œéœ€è¦æ›´åŠ æ¸…æ™°æœ‰åŠ›çš„å‘å£°")
            elif features['energy_mean'] < 0.01:
                score -= 15
                issues.append(f"éŸ³ç´ '{phoneme}'å‘éŸ³èƒ½é‡è¾ƒä½")
        
        # æ›´ä¸¥æ ¼çš„é¢‘è°±è´¨å¿ƒè¯„ä¼°ï¼ˆç”¨äºåˆ¤æ–­æ¸…æ™°åº¦ï¼‰
        if 'spectral_centroid_mean' in features:
            # ä¸åŒéŸ³ç´ æœ‰ä¸åŒçš„é¢‘è°±ç‰¹å¾æœŸæœ›å€¼
            if phoneme in ['s', 'Êƒ', 'f', 'Î¸']:  # é«˜é¢‘æ‘©æ“¦éŸ³
                if features['spectral_centroid_mean'] < 2500:  # æé«˜æ ‡å‡†
                    score -= 20
                    issues.append(f"é«˜é¢‘æ‘©æ“¦éŸ³'{phoneme}'é«˜é¢‘æˆåˆ†ä¸è¶³ï¼Œéœ€è¦æ›´æ˜æ˜¾çš„æ‘©æ“¦å£°")
            elif phoneme in ['Ã¦', 'É‘Ë', 'É’']:  # ä½é¢‘å…ƒéŸ³
                if features['spectral_centroid_mean'] > 1500:  # æé«˜æ ‡å‡†
                    score -= 15
                    issues.append(f"ä½é¢‘å…ƒéŸ³'{phoneme}'éŸ³è‰²åé«˜ï¼Œéœ€è¦æ›´ä½çš„èˆŒä½")
            elif phoneme in ['iË', 'Éª']:  # é«˜å…ƒéŸ³
                if features['spectral_centroid_mean'] < 1000 or features['spectral_centroid_mean'] > 2200:
                    score -= 15
                    issues.append(f"é«˜å…ƒéŸ³'{phoneme}'èˆŒä½ä¸å‡†ç¡®ï¼Œéœ€è¦è°ƒæ•´å£å‹")
        
        # æ–°å¢ï¼šæ›´ç»†è‡´çš„éŸ³ç´ ç±»åˆ«æ£€æŸ¥
        phoneme_type = self.classify_phoneme_detailed(phoneme)
        type_issues = self.check_phoneme_type_quality(phoneme, phoneme_type, features, duration)
        issues.extend(type_issues)
        score -= len(type_issues) * 8  # æ¯ä¸ªç±»å‹é—®é¢˜æ‰£8åˆ†
        
        return max(0, min(100, score)), issues
    
    def classify_phoneme_detailed(self, phoneme: str) -> str:
        """æ›´è¯¦ç»†çš„éŸ³ç´ åˆ†ç±»"""
        vowels = ['Ã¦', 'Éª', 'ÊŠ', 'iË', 'uË', 'ÉœË', 'ÊŒ', 'aÉª', 'aÊŠ', 'É”Éª', 'e', 'É’', 'É‘Ë']
        fricatives = ['f', 'v', 's', 'z', 'Êƒ', 'Ê’', 'Î¸', 'Ã°', 'h']
        stops = ['p', 'b', 't', 'd', 'k', 'g']
        affricates = ['tÊƒ', 'dÊ’']
        nasals = ['m', 'n', 'Å‹']
        liquids = ['l', 'r']
        glides = ['w', 'j']
        
        if phoneme in vowels:
            # è¿›ä¸€æ­¥ç»†åˆ†å…ƒéŸ³
            if phoneme in ['Ã¦', 'É‘Ë', 'É’']:
                return 'low_vowel'  # ä½å…ƒéŸ³
            elif phoneme in ['iË', 'Éª', 'uË', 'ÊŠ']:
                return 'high_vowel'  # é«˜å…ƒéŸ³
            elif phoneme in ['e', 'ÉœË', 'ÊŒ']:
                return 'mid_vowel'  # ä¸­å…ƒéŸ³
            else:
                return 'diphthong'  # åŒå…ƒéŸ³
        elif phoneme in fricatives:
            if phoneme in ['s', 'z', 'Êƒ', 'Ê’']:
                return 'sibilant_fricative'  # å–™éŸ³æ‘©æ“¦éŸ³
            else:
                return 'non_sibilant_fricative'  # éå–™éŸ³æ‘©æ“¦éŸ³
        elif phoneme in stops:
            if phoneme in ['p', 't', 'k']:
                return 'voiceless_stop'  # æ¸…éŸ³çˆ†ç ´éŸ³
            else:
                return 'voiced_stop'  # æµŠéŸ³çˆ†ç ´éŸ³
        elif phoneme in affricates:
            return 'affricate'  # å¡æ“¦éŸ³
        elif phoneme in nasals:
            return 'nasal'
        elif phoneme in liquids:
            return 'liquid'
        elif phoneme in glides:
            return 'glide'
        else:
            return 'unknown'
    
    def check_phoneme_type_quality(self, phoneme: str, phoneme_type: str, features: Dict, duration: float) -> List[str]:
        """æ£€æŸ¥ç‰¹å®šéŸ³ç´ ç±»å‹çš„è´¨é‡é—®é¢˜"""
        issues = []
        
        # ä½å…ƒéŸ³æ£€æŸ¥
        if phoneme_type == 'low_vowel':
            if 'f1' in features and features['f1'] > 0:
                if features['f1'] < 600:  # F1åº”è¯¥è¾ƒé«˜
                    issues.append(f"ä½å…ƒéŸ³'{phoneme}'å¼€å£åº¦ä¸å¤Ÿï¼Œéœ€è¦æ›´å¤§çš„å¼ å£")
        
        # é«˜å…ƒéŸ³æ£€æŸ¥
        elif phoneme_type == 'high_vowel':
            if 'f1' in features and features['f1'] > 0:
                if features['f1'] > 450:  # F1åº”è¯¥è¾ƒä½
                    issues.append(f"é«˜å…ƒéŸ³'{phoneme}'èˆŒä½è¿‡ä½ï¼Œéœ€è¦æé«˜èˆŒä½")
        
        # å–™éŸ³æ‘©æ“¦éŸ³æ£€æŸ¥
        elif phoneme_type == 'sibilant_fricative':
            if 'zcr_mean' in features:
                if features['zcr_mean'] < 0.15:  # é›¶äº¤å‰ç‡åº”è¯¥è¾ƒé«˜
                    issues.append(f"å–™éŸ³'{phoneme}'æ‘©æ“¦å£°ä¸å¤Ÿæ˜æ˜¾ï¼Œéœ€è¦æ›´æ˜æ˜¾çš„æ°”æµæ‘©æ“¦")
        
        # æ¸…éŸ³çˆ†ç ´éŸ³æ£€æŸ¥
        elif phoneme_type == 'voiceless_stop':
            if 'energy_max' in features and 'energy_mean' in features:
                if features['energy_mean'] > 0:
                    energy_ratio = features['energy_max'] / features['energy_mean']
                    if energy_ratio < 2.5:  # çˆ†ç ´ç‰¹å¾ä¸æ˜æ˜¾
                        issues.append(f"æ¸…éŸ³çˆ†ç ´éŸ³'{phoneme}'çˆ†ç ´ç‰¹å¾ä¸æ˜æ˜¾ï¼Œéœ€è¦æ›´æ˜æ˜¾çš„çˆ†ç ´å£´")
        
        # æµŠéŸ³çˆ†ç ´éŸ³æ£€æŸ¥
        elif phoneme_type == 'voiced_stop':
            if 'voicing_rate' in features:
                if features['voicing_rate'] < 0.6:  # æµŠéŸ³ç¨‹åº¦ä¸è¶³
                    issues.append(f"æµŠéŸ³çˆ†ç ´éŸ³'{phoneme}'æµŠéŸ³ç¨‹åº¦ä¸è¶³ï¼Œéœ€è¦æ›´å¤šå£°å¸¦æŒ¯åŠ¨")
        
        # é¼»éŸ³æ£€æŸ¥
        elif phoneme_type == 'nasal':
            if 'f1' in features and features['f1'] > 0:
                if features['f1'] > 500:  # é¼»éŸ³F1åº”è¯¥è¾ƒä½
                    issues.append(f"é¼»éŸ³'{phoneme}'é¼»è…”å…±é¸£ä¸è¶³ï¼Œæ³¨æ„è½¯è…Šä¸‹é™")
        
        return issues
    
    def get_quality_level(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†è·å–è´¨é‡ç­‰çº§"""
        if score >= 90:
            return "excellent"
        elif score >= 75:
            return "good"
        elif score >= 60:
            return "fair"
        else:
            return "poor"
    
    def analyze_pronunciation_detailed(self, audio_data: np.ndarray, reference_text: str,
                                     wav2vec2_model, processor, sr: int = 16000) -> DetailedPronunciationResult:
        """æ‰§è¡Œè¯¦ç»†çš„å‘éŸ³åˆ†æ"""
        try:
            print(f"å¼€å§‹éŸ³ç´ çº§å‘éŸ³åˆ†æ: '{reference_text}'")
            
            # 1. æ–‡æœ¬è½¬éŸ³ç´ 
            phoneme_sequence = self.text_to_phonemes(reference_text)
            print(f"éŸ³ç´ åºåˆ—: {phoneme_sequence}")
            
            # 2. å•è¯åˆ†å‰²å’ŒéŸ³ç´ æ˜ å°„
            words = reference_text.lower().split()
            word_phoneme_mapping = self.map_words_to_phonemes(words)
            print(f"å•è¯éŸ³ç´ æ˜ å°„: {word_phoneme_mapping}")
            
            # 3. å¼ºåˆ¶å¯¹é½
            alignments = self.force_align_ctc(audio_data, phoneme_sequence, wav2vec2_model, processor, sr)
            print(f"å¯¹é½ç»“æœæ•°é‡: {len(alignments)}")
            
            # 4. æå–æ•´ä½“å£°å­¦ç‰¹å¾
            global_features = self.extract_acoustic_features(audio_data, sr)
            
            # 5. éŸ³ç´ çº§è¯„åˆ†
            phoneme_scores = []
            word_scores = []
            all_issues = []
            
            for phoneme, start_time, end_time in alignments:
                # æå–è¯¥éŸ³ç´ å¯¹åº”çš„éŸ³é¢‘æ®µ
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                phoneme_audio = audio_data[start_sample:end_sample]
                
                if len(phoneme_audio) > 0:
                    # æå–éŸ³ç´ çº§ç‰¹å¾
                    phoneme_features = self.extract_acoustic_features(phoneme_audio, sr)
                    
                    # è¯„åˆ†
                    duration = end_time - start_time
                    score, issues = self.score_phoneme_quality(phoneme, phoneme_features, duration)
                    
                    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç‰¹å¾ç¨³å®šæ€§ï¼‰
                    confidence = min(1.0, max(0.3, 1.0 - len(issues) * 0.1))
                    
                    phoneme_score = PhonemeScore(
                        phoneme=phoneme,
                        start_time=start_time,
                        end_time=end_time,
                        score=score,
                        confidence=confidence,
                        quality=self.get_quality_level(score),
                        issues=issues
                    )
                    
                    phoneme_scores.append(phoneme_score)
                    all_issues.extend(issues)
            
            # 6. å•è¯çº§è¯„åˆ†å’Œåˆ†æ
            word_scores = self.analyze_word_pronunciation(words, word_phoneme_mapping, phoneme_scores)
            
            # 7. è®¡ç®—æ€»åˆ†ï¼ˆæ›´åŠ ä¸¥æ ¼çš„è¯„åˆ†æ ‡å‡†ï¼‰
            if phoneme_scores:
                # åŠ æƒå¹³å‡ï¼Œè€ƒè™‘ç½®ä¿¡åº¦å’Œè´¨é‡ç­‰çº§
                weighted_scores = []
                quality_weights = {'excellent': 1.0, 'good': 0.9, 'fair': 0.7, 'poor': 0.4}
                
                for ps in phoneme_scores:
                    # ç»“åˆè¯„åˆ†ã€ç½®ä¿¡åº¦å’Œè´¨é‡ç­‰çº§
                    quality_weight = quality_weights.get(ps.quality, 0.5)
                    adjusted_score = ps.score * ps.confidence * quality_weight
                    weighted_scores.append(adjusted_score)
                
                # è®¡ç®—åŸºç¡€å¹³å‡åˆ†
                base_score = np.mean(weighted_scores) if weighted_scores else 0
                
                # åº”ç”¨é¢å¤–çš„ä¸¥æ ¼åº¦æƒ©ç½š
                penalty = 0
                
                # é—®é¢˜éŸ³ç´ æ¯”ä¾‹æƒ©ç½š
                poor_ratio = len([ps for ps in phoneme_scores if ps.quality == 'poor']) / len(phoneme_scores)
                fair_ratio = len([ps for ps in phoneme_scores if ps.quality == 'fair']) / len(phoneme_scores)
                
                if poor_ratio > 0.3:  # è¶…è¿‡30%çš„éŸ³ç´ è´¨é‡å·®
                    penalty += 15
                elif poor_ratio > 0.2:
                    penalty += 10
                
                if fair_ratio > 0.4:  # è¶…è¿‡40%çš„éŸ³ç´ è´¨é‡ä¸€èˆ¬
                    penalty += 8
                
                # é—®é¢˜æ•°é‡æƒ©ç½š
                total_issues = len(all_issues)
                if total_issues > len(phoneme_scores) * 0.5:  # å¹³å‡æ¯ä¸ªéŸ³ç´ è¶…è¿‡0.5ä¸ªé—®é¢˜
                    penalty += 12
                elif total_issues > len(phoneme_scores) * 0.3:
                    penalty += 6
                
                # ä¸¥é‡é—®é¢˜é¢å¤–æƒ©ç½š
                severe_issues = [issue for issue in all_issues if 'è¿‡çŸ­' in issue or 'è¿‡é•¿' in issue or 'èƒ½é‡ä¸è¶³' in issue]
                if len(severe_issues) > 3:
                    penalty += 10
                
                # è®¡ç®—æœ€ç»ˆè¯„åˆ†
                overall_score = max(0, min(100, base_score - penalty))
                
                # å¦‚æœè¯„åˆ†ä»ç„¶è¿‡é«˜ï¼Œé¢å¤–è°ƒæ•´
                if overall_score > 85 and (poor_ratio > 0.1 or len(severe_issues) > 1):
                    overall_score = min(85, overall_score - 5)
                
                if overall_score > 75 and poor_ratio > 0.2:
                    overall_score = min(75, overall_score - 5)
            else:
                overall_score = 0
            
            # 8. åˆ†æè¯­è°ƒå’Œæ—¶é•¿
            duration_analysis = {
                'total_duration': len(audio_data) / sr,
                'speech_rate': len(phoneme_sequence) / (len(audio_data) / sr) if len(audio_data) > 0 else 0,
                'avg_phoneme_duration': np.mean([ps.end_time - ps.start_time for ps in phoneme_scores]) if phoneme_scores else 0
            }
            
            pitch_analysis = {}
            if 'f0_mean' in global_features:
                pitch_analysis = {
                    'average_f0': global_features['f0_mean'],
                    'f0_variation': global_features['f0_std'],
                    'pitch_range': 'normal'  # éœ€è¦æ›´å¤æ‚çš„åˆ†æ
                }
            
            # 9. ç”Ÿæˆæ”¹è¿›å»ºè®®ï¼ˆåŒ…æ‹¬å•è¯çº§å»ºè®®ï¼‰
            improvement_suggestions = self._generate_detailed_suggestions(
                phoneme_scores, word_scores, all_issues, reference_text
            )
            
            result = DetailedPronunciationResult(
                overall_score=overall_score,
                phoneme_scores=phoneme_scores,
                word_scores=word_scores,
                pronunciation_issues=list(set(all_issues)),
                improvement_suggestions=improvement_suggestions,
                duration_analysis=duration_analysis,
                pitch_analysis=pitch_analysis
            )
            
            print(f"éŸ³ç´ çº§åˆ†æå®Œæˆï¼Œæ€»åˆ†: {overall_score:.1f}")
            return result
            
        except Exception as e:
            print(f"éŸ³ç´ çº§åˆ†æå¤±è´¥: {e}")
            traceback.print_exc()
            
            # è¿”å›é»˜è®¤ç»“æœ
            return DetailedPronunciationResult(
                overall_score=0,
                phoneme_scores=[],
                word_scores=[],
                pronunciation_issues=[f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"],
                improvement_suggestions=["è¯·æ£€æŸ¥éŸ³é¢‘è´¨é‡å¹¶é‡æ–°å½•éŸ³"],
                duration_analysis={'total_duration': len(audio_data) / sr if len(audio_data) > 0 else 0},
                pitch_analysis={}
            )
    
    def _generate_detailed_suggestions(self, phoneme_scores: List[PhonemeScore], word_scores: List[Dict], 
                                     all_issues: List[str], reference_text: str) -> List[str]:
        """ç”ŸæˆåŒ…å«å•è¯çº§å»ºè®®çš„è¯¦ç»†æ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # 1. å•è¯çº§åˆ«çš„å»ºè®®ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼‰
        words_needing_improvement = [ws for ws in word_scores if ws.get('needs_improvement', False)]
        if words_needing_improvement:
            suggestions.append("ğŸ¯ éœ€è¦é‡ç‚¹æ”¹è¿›çš„å•è¯:")
            for word_info in words_needing_improvement[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                word = word_info['word']
                score = word_info['score']
                quality = word_info['quality']
                quality_desc = {'excellent': 'ä¼˜ç§€', 'good': 'è‰¯å¥½', 'fair': 'ä¸€èˆ¬', 'poor': 'è¾ƒå·®', 'unknown': 'æœªçŸ¥'}[quality]
                suggestions.append(f"  â€¢ '{word}' (è¯„åˆ†: {score}, è´¨é‡: {quality_desc})")
                
                # æ·»åŠ è¯¥å•è¯çš„å…·ä½“å»ºè®®
                word_suggestions = word_info.get('suggestions', [])
                for ws in word_suggestions[:2]:  # æ¯ä¸ªå•è¯æœ€å¤š2ä¸ªå»ºè®®
                    suggestions.append(f"    - {ws}")
        
        # 2. éŸ³ç´ çº§åˆ«çš„æ€»ä½“é—®é¢˜åˆ†æ
        poor_phonemes = [ps.phoneme for ps in phoneme_scores if ps.quality == 'poor']
        fair_phonemes = [ps.phoneme for ps in phoneme_scores if ps.quality == 'fair']
        
        if poor_phonemes:
            unique_poor = list(set(poor_phonemes))
            suggestions.append(f"\nğŸ” éœ€è¦é‡ç‚¹ç»ƒä¹ çš„éŸ³ç´ : {', '.join(unique_poor[:6])}")
        
        if fair_phonemes and not poor_phonemes:
            unique_fair = list(set(fair_phonemes))
            suggestions.append(f"\nğŸ“ˆ å¯ä»¥è¿›ä¸€æ­¥å®Œå–„çš„éŸ³ç´ : {', '.join(unique_fair[:4])}")
        
        # 3. æŒ‰é—®é¢˜ç±»å‹åˆ†ç±»å»ºè®®
        timing_issues = [issue for issue in all_issues if ('è¿‡çŸ­' in issue or 'è¿‡é•¿' in issue)]
        clarity_issues = [issue for issue in all_issues if ('ä¸ç¨³å®š' in issue or 'æ¸…æ™°åº¦' in issue or 'èƒ½é‡ä¸è¶³' in issue)]
        articulation_issues = [issue for issue in all_issues if ('æ‘©æ“¦' in issue or 'çˆ†ç ´' in issue or 'èˆŒä½' in issue or 'å¼€å£' in issue)]
        
        if timing_issues:
            suggestions.append("\nâ° æ—¶é•¿æ§åˆ¶å»ºè®®:")
            suggestions.append("  â€¢ æ³¨æ„æ§åˆ¶æ¯ä¸ªéŸ³ç´ çš„å‘éŸ³æ—¶é•¿ï¼Œé¿å…è¿‡å¿«æˆ–è¿‡æ…¢")
            suggestions.append("  â€¢ å¯ä»¥å°è¯•è·Ÿè¯»æ ‡å‡†éŸ³é¢‘ï¼ŒåŸ¹å…»èŠ‚å¥æ„Ÿ")
        
        if clarity_issues:
            suggestions.append("\nğŸ¤ æ¸…æ™°åº¦æ”¹è¿›å»ºè®®:")
            suggestions.append("  â€¢ æé«˜å‘éŸ³æ¸…æ™°åº¦ï¼Œç¡®ä¿æ¯ä¸ªéŸ³ç´ éƒ½å‘éŸ³å……åˆ†")
            suggestions.append("  â€¢ ç»ƒä¹ æ—¶å¯ä»¥é€‚å½“æ”¾æ…¢è¯­é€Ÿï¼Œä¿è¯å‡†ç¡®æ€§")
        
        if articulation_issues:
            suggestions.append("\nğŸ‘„ å‘éŸ³æŠ€å·§å»ºè®®:")
            suggestions.append("  â€¢ æ³¨æ„å£å‹å’ŒèˆŒä½çš„å‡†ç¡®æ€§")
            suggestions.append("  â€¢ å¯¹ç…§é•œå­ç»ƒä¹ ï¼Œè§‚å¯Ÿå£å‹å˜åŒ–")
        
        # 4. æ•´ä½“ç»ƒä¹ å»ºè®®
        overall_score = np.mean([ps.score for ps in phoneme_scores]) if phoneme_scores else 0
        
        if overall_score < 60:
            suggestions.append("\nğŸ“š åŸºç¡€ç»ƒä¹ å»ºè®®:")
            suggestions.append("  â€¢ å»ºè®®ä»åŸºç¡€éŸ³ç´ å¼€å§‹ï¼Œé€ä¸ªæ”»å…‹")
            suggestions.append("  â€¢ å¤šå¬æ ‡å‡†å‘éŸ³ï¼ŒåŸ¹å…»è¯­æ„Ÿ")
            suggestions.append("  â€¢ æ¯å¤©åšæŒç»ƒä¹ 15-20åˆ†é’Ÿ")
        elif overall_score < 75:
            suggestions.append("\nğŸ¯ æå‡ç»ƒä¹ å»ºè®®:")
            suggestions.append("  â€¢ é‡ç‚¹å…³æ³¨ä¸Šè¿°é—®é¢˜éŸ³ç´ å’Œå•è¯")
            suggestions.append("  â€¢ å¯ä»¥å°è¯•å½•éŸ³å¯¹æ¯”ï¼Œæ‰¾å‡ºå·®è·")
        elif overall_score < 85:
            suggestions.append("\nâœ¨ ç²¾è¿›ç»ƒä¹ å»ºè®®:")
            suggestions.append("  â€¢ ç»§ç»­ä¿æŒç»ƒä¹ ï¼Œæ³¨æ„ç»†èŠ‚å®Œå–„")
            suggestions.append("  â€¢ å¯ä»¥å°è¯•æ›´å…·æŒ‘æˆ˜æ€§çš„å¥å­")
        else:
            suggestions.append("\nğŸŒŸ ç»§ç»­ä¿æŒ:")
            suggestions.append("  â€¢ å‘éŸ³æ°´å¹³å¾ˆå¥½ï¼Œç»§ç»­ä¿æŒï¼")
            suggestions.append("  â€¢ å¯ä»¥å°è¯•æ›´å¤æ‚çš„è¯­éŸ³ç»ƒä¹ ")
        
        # 5. å®ç”¨ç»ƒä¹ æŠ€å·§
        if len(all_issues) > 3:
            suggestions.append("\nğŸ’¡ ç»ƒä¹ æŠ€å·§æç¤º:")
            suggestions.append("  â€¢ å»ºè®®ä½¿ç”¨æ…¢é€Ÿæ’­æ”¾åŠŸèƒ½ä»”ç»†å¬æ ‡å‡†å‘éŸ³")
            suggestions.append("  â€¢ å¯ä»¥åˆ†æ®µç»ƒä¹ ï¼Œå…ˆæŒæ¡å•ä¸ªå•è¯å†è¿æ¥æˆå¥")
            suggestions.append("  â€¢ å½•åˆ¶è‡ªå·±çš„å‘éŸ³å¹¶ä¸æ ‡å‡†å‘éŸ³å¯¹æ¯”")
        
        return suggestions
    
    def map_words_to_phonemes(self, words: List[str]) -> Dict[str, List[str]]:
        """å°†å•è¯æ˜ å°„åˆ°å¯¹åº”çš„éŸ³ç´ åºåˆ—"""
        word_phoneme_mapping = {}
        
        # æ‰©å±•çš„éŸ³ç´ è§„åˆ™ï¼ŒåŒ…å«æ›´å¤šå¸¸ç”¨å•è¯
        phoneme_rules = {
            'the': ['Ã°', 'É™'],
            'and': ['Ã¦', 'n', 'd'],
            'have': ['h', 'Ã¦', 'v'],
            'that': ['Ã°', 'Ã¦', 't'],
            'for': ['f', 'É”Ë'],
            'are': ['É‘Ë'],
            'with': ['w', 'Éª', 'Î¸'],
            'his': ['h', 'Éª', 'z'],
            'they': ['Ã°', 'eÉª'],
            'this': ['Ã°', 'Éª', 's'],
            'from': ['f', 'r', 'É’', 'm'],
            'she': ['Êƒ', 'iË'],
            'her': ['h', 'ÉœË'],
            'been': ['b', 'iË', 'n'],
            'than': ['Ã°', 'Ã¦', 'n'],
            'its': ['Éª', 't', 's'],
            'now': ['n', 'aÊŠ'],
            'more': ['m', 'É”Ë'],
            'very': ['v', 'e', 'r', 'Éª'],
            'what': ['w', 'É’', 't'],
            'know': ['n', 'É™ÊŠ'],
            'just': ['dÊ’', 'ÊŒ', 's', 't'],
            'first': ['f', 'ÉœË', 's', 't'],
            'time': ['t', 'aÉª', 'm'],
            'people': ['p', 'iË', 'p', 'É™', 'l'],
            'good': ['g', 'ÊŠ', 'd'],
            'work': ['w', 'ÉœË', 'k'],
            'school': ['s', 'k', 'uË', 'l'],
            'world': ['w', 'ÉœË', 'l', 'd'],
            'great': ['g', 'r', 'eÉª', 't'],
            'think': ['Î¸', 'Éª', 'Å‹', 'k'],
            'way': ['w', 'eÉª'],
            'make': ['m', 'eÉª', 'k'],
            'today': ['t', 'É™', 'd', 'eÉª'],
            'help': ['h', 'e', 'l', 'p'],
            'home': ['h', 'É™ÊŠ', 'm'],
            'nice': ['n', 'aÉª', 's'],
            'happy': ['h', 'Ã¦', 'p', 'Éª'],
            'love': ['l', 'ÊŒ', 'v'],
            'like': ['l', 'aÉª', 'k'],
            'want': ['w', 'É’', 'n', 't'],
            'need': ['n', 'iË', 'd'],
            'thank': ['Î¸', 'Ã¦', 'Å‹', 'k'],
            'you': ['j', 'uË'],
            'hello': ['h', 'É™', 'l', 'É™ÊŠ'],
            'water': ['w', 'É”Ë', 't', 'É™r'],
            'food': ['f', 'uË', 'd'],
            'money': ['m', 'ÊŒ', 'n', 'Éª'],
            'house': ['h', 'aÊŠ', 's'],
            'friend': ['f', 'r', 'e', 'n', 'd'],
            'family': ['f', 'Ã¦', 'm', 'Éª', 'l', 'Éª'],
            'book': ['b', 'ÊŠ', 'k'],
            'music': ['m', 'j', 'uË', 'z', 'Éª', 'k'],
            'beautiful': ['b', 'j', 'uË', 't', 'Éª', 'f', 'ÊŠ', 'l'],
            'important': ['Éª', 'm', 'p', 'É”Ë', 't', 'É™', 'n', 't'],
            'different': ['d', 'Éª', 'f', 'É™r', 'É™', 'n', 't'],
            'because': ['b', 'Éª', 'k', 'É’', 'z']
        }
        
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word.lower())
            if word_clean in phoneme_rules:
                word_phoneme_mapping[word_clean] = phoneme_rules[word_clean]
            else:
                # å¯¹æœªçŸ¥å•è¯ä½¿ç”¨ç®€å•çš„å­—æ¯åˆ°éŸ³ç´ æ˜ å°„
                phonemes = []
                for char in word_clean:
                    if char in self.phoneme_map:
                        phonemes.append(self.phoneme_map[char])
                    else:
                        phonemes.append(char)
                word_phoneme_mapping[word_clean] = phonemes
        
        return word_phoneme_mapping
    
    def analyze_word_pronunciation(self, words: List[str], word_phoneme_mapping: Dict[str, List[str]], 
                                 phoneme_scores: List[PhonemeScore]) -> List[Dict]:
        """åˆ†æå•è¯çº§å‘éŸ³è´¨é‡"""
        word_scores = []
        
        # åˆ›å»ºéŸ³ç´ åˆ°è¯„åˆ†çš„æ˜ å°„
        phoneme_score_map = {}
        for ps in phoneme_scores:
            if ps.phoneme not in phoneme_score_map:
                phoneme_score_map[ps.phoneme] = []
            phoneme_score_map[ps.phoneme].append(ps)
        
        # è·Ÿè¸ªå½“å‰éŸ³ç´ ä½ç½®
        current_phoneme_index = 0
        
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word.lower())
            
            if word_clean in word_phoneme_mapping:
                word_phonemes = word_phoneme_mapping[word_clean]
                word_phoneme_scores = []
                word_issues = []
                
                # æ”¶é›†è¯¥å•è¯å¯¹åº”çš„éŸ³ç´ è¯„åˆ†
                for phoneme in word_phonemes:
                    if current_phoneme_index < len(phoneme_scores):
                        # å¯»æ‰¾åŒ¹é…çš„éŸ³ç´ è¯„åˆ†
                        found_score = None
                        
                        # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…å½“å‰ä½ç½®
                        if (current_phoneme_index < len(phoneme_scores) and 
                            phoneme_scores[current_phoneme_index].phoneme == phoneme):
                            found_score = phoneme_scores[current_phoneme_index]
                            current_phoneme_index += 1
                        else:
                            # åœ¨é™„è¿‘ä½ç½®æœç´¢åŒ¹é…çš„éŸ³ç´ 
                            for i in range(max(0, current_phoneme_index - 2), 
                                         min(len(phoneme_scores), current_phoneme_index + 3)):
                                if phoneme_scores[i].phoneme == phoneme:
                                    found_score = phoneme_scores[i]
                                    current_phoneme_index = i + 1
                                    break
                        
                        if found_score:
                            word_phoneme_scores.append(found_score)
                            word_issues.extend(found_score.issues)
                
                # è®¡ç®—å•è¯è¯„åˆ†
                if word_phoneme_scores:
                    # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œè´¨é‡å·®çš„éŸ³ç´ æƒé‡æ›´é«˜ï¼ˆå½±å“æ›´å¤§ï¼‰
                    scores = [ps.score for ps in word_phoneme_scores]
                    qualities = [ps.quality for ps in word_phoneme_scores]
                    
                    # è´¨é‡æƒé‡ï¼šå·®çš„éŸ³ç´ å¯¹æ•´ä½“å½±å“æ›´å¤§
                    quality_weights = {'excellent': 1.0, 'good': 1.1, 'fair': 1.3, 'poor': 1.5}
                    weighted_scores = []
                    
                    for ps in word_phoneme_scores:
                        weight = quality_weights.get(ps.quality, 1.0)
                        weighted_scores.append(ps.score / weight)  # è´¨é‡è¶Šå·®ï¼Œåˆ†æ•°å½±å“è¶Šå¤§
                    
                    word_score = np.mean(weighted_scores)
                    
                    # æ ¹æ®é—®é¢˜æ•°é‡è¿›ä¸€æ­¥è°ƒæ•´
                    severe_issues = [issue for issue in word_issues if 
                                   ('è¿‡çŸ­' in issue or 'è¿‡é•¿' in issue or 'èƒ½é‡ä¸è¶³' in issue or 'ä¸ç¨³å®š' in issue)]
                    if len(severe_issues) > len(word_phonemes) * 0.3:
                        word_score *= 0.85  # ä¸¥é‡é—®é¢˜è¾ƒå¤šæ—¶é™åˆ†
                    
                    # ç¡®å®šå•è¯è´¨é‡ç­‰çº§
                    word_quality = self.get_quality_level(word_score)
                    
                    # ç”Ÿæˆå•è¯çº§å»ºè®®
                    word_suggestions = self._generate_word_suggestions(word_clean, word_phoneme_scores, word_issues)
                    
                    word_analysis = {
                        'word': word_clean,
                        'score': round(word_score, 1),
                        'quality': word_quality,
                        'phonemes': word_phonemes,
                        'phoneme_scores': [{
                            'phoneme': ps.phoneme,
                            'score': ps.score,
                            'quality': ps.quality,
                            'issues': ps.issues
                        } for ps in word_phoneme_scores],
                        'issues': list(set(word_issues)),
                        'suggestions': word_suggestions,
                        'needs_improvement': word_score < 70 or len(severe_issues) > 0
                    }
                    
                    word_scores.append(word_analysis)
                else:
                    # æ— æ³•æ‰¾åˆ°å¯¹åº”çš„éŸ³ç´ è¯„åˆ†
                    word_scores.append({
                        'word': word_clean,
                        'score': 0,
                        'quality': 'unknown',
                        'phonemes': word_phonemes,
                        'phoneme_scores': [],
                        'issues': ['æ— æ³•è·å–è¯¥å•è¯çš„è¯¦ç»†å‘éŸ³åˆ†æ'],
                        'suggestions': [f"è¯·é‡ç‚¹ç»ƒä¹ å•è¯ '{word_clean}' çš„å‘éŸ³"],
                        'needs_improvement': True
                    })
        
        return word_scores
    
    def _generate_word_suggestions(self, word: str, phoneme_scores: List[PhonemeScore], issues: List[str]) -> List[str]:
        """ä¸ºç‰¹å®šå•è¯ç”Ÿæˆå‘éŸ³æ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åˆ†æå•è¯ä¸­çš„é—®é¢˜éŸ³ç´ 
        poor_phonemes = [ps.phoneme for ps in phoneme_scores if ps.quality == 'poor']
        fair_phonemes = [ps.phoneme for ps in phoneme_scores if ps.quality == 'fair']
        
        # æ—¶é•¿é—®é¢˜
        timing_issues = [issue for issue in issues if ('è¿‡çŸ­' in issue or 'è¿‡é•¿' in issue)]
        if timing_issues:
            suggestions.append(f"å•è¯ '{word}' çš„å‘éŸ³èŠ‚å¥éœ€è¦è°ƒæ•´ï¼Œæ³¨æ„æ¯ä¸ªéŸ³ç´ çš„æ—¶é•¿")
        
        # æ¸…æ™°åº¦é—®é¢˜
        clarity_issues = [issue for issue in issues if ('ä¸ç¨³å®š' in issue or 'æ¸…æ™°' in issue or 'èƒ½é‡ä¸è¶³' in issue)]
        if clarity_issues:
            suggestions.append(f"å•è¯ '{word}' éœ€è¦æ›´æ¸…æ™°çš„å‘éŸ³ï¼Œæ³¨æ„å£å‹å’Œå‘å£°")
        
        # ç‰¹å®šéŸ³ç´ é—®é¢˜
        if poor_phonemes:
            problematic_phonemes = ', '.join(list(set(poor_phonemes)))
            suggestions.append(f"å•è¯ '{word}' ä¸­çš„éŸ³ç´  [{problematic_phonemes}] éœ€è¦é‡ç‚¹ç»ƒä¹ ")
        
        if fair_phonemes and not poor_phonemes:  # åªæœ‰ä¸€èˆ¬é—®é¢˜æ—¶
            suggestions.append(f"å•è¯ '{word}' çš„å‘éŸ³åŸºæœ¬æ­£ç¡®ï¼Œå¯ä»¥è¿›ä¸€æ­¥å®Œå–„")
        
        # å•è¯ç‰¹å®šçš„å‘éŸ³å»ºè®®
        word_specific_suggestions = self._get_word_specific_suggestions(word, poor_phonemes + fair_phonemes)
        suggestions.extend(word_specific_suggestions)
        
        # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œç»™å‡ºé¼“åŠ±
        if not poor_phonemes and not fair_phonemes and not timing_issues and not clarity_issues:
            suggestions.append(f"å•è¯ '{word}' çš„å‘éŸ³å¾ˆå¥½ï¼")
        
        return suggestions
    
    def _get_word_specific_suggestions(self, word: str, problematic_phonemes: List[str]) -> List[str]:
        """è·å–ç‰¹å®šå•è¯çš„å‘éŸ³æŠ€å·§å»ºè®®"""
        suggestions = []
        
        # å¸¸è§å•è¯çš„ç‰¹æ®Šå‘éŸ³å»ºè®®
        word_tips = {
            'the': "æ³¨æ„ 'th' çš„å‘éŸ³ï¼ŒèˆŒå°–è½»è§¦ä¸Šé½¿",
            'this': "'th' å’Œ 'is' è¦è¿æ¥è‡ªç„¶ï¼Œä¸è¦åœé¡¿",
            'that': "å¼ºè°ƒ 'th' çš„æ‘©æ“¦éŸ³å’Œ 'a' çš„å¼€å£åº¦",
            'think': "'th' å’Œ 'ink' çš„è¿æ¥è¦æµç•…",
            'thank': "'th' + 'ank'ï¼Œæ³¨æ„é¼»éŸ³ 'n'",
            'with': "'w' è¦åœ†å”‡ï¼Œ'th' è¦æ¸…æ™°",
            'water': "æ³¨æ„ 'wa' çš„åŒå”‡æ”¶ç¼©å’Œ 'ter' çš„å·èˆŒéŸ³",
            'work': "'w' åœ†å”‡å¼€å§‹ï¼Œ'or' è¦æœ‰é€‚å½“çš„å…ƒéŸ³é•¿åº¦",
            'world': "'w' + 'or' + 'ld'ï¼Œæœ€åçš„ 'ld' è¦æ¸…æ™°",
            'beautiful': "æ³¨æ„é‡éŸ³åœ¨ 'beau'ï¼Œ'ti' è¦è½»è¯»",
            'important': "é‡éŸ³åœ¨ 'por'ï¼Œæ³¨æ„æ¯ä¸ªéŸ³èŠ‚çš„æ¸…æ™°åº¦",
            'different': "'dif' + 'fer' + 'ent'ï¼Œæ³¨æ„é‡éŸ³åˆ†é…",
            'school': "'sch' è¦è¿ç»­å‘éŸ³ï¼Œ'ool' è¦æ‹–é•¿",
            'people': "'peo' + 'ple'ï¼Œæ³¨æ„æœ€åçš„è½»éŸ³èŠ‚",
            'because': "'be' + 'cause'ï¼Œé‡éŸ³åœ¨ 'cause'"
        }
        
        if word in word_tips:
            suggestions.append(word_tips[word])
        
        # æ ¹æ®é—®é¢˜éŸ³ç´ ç»™å‡ºå»ºè®®
        for phoneme in problematic_phonemes:
            if phoneme == 'Î¸':  # théŸ³
                suggestions.append("ç»ƒä¹  'th' éŸ³ï¼šèˆŒå°–è½»è§¦ä¸Šé½¿ï¼Œæ°”æµä»èˆŒé½¿é—´é€šè¿‡")
            elif phoneme == 'Ã°':  # æµŠthéŸ³
                suggestions.append("æµŠ 'th' éŸ³è¦æœ‰å£°å¸¦æŒ¯åŠ¨ï¼Œä¸æ¸… 'th' åŒºåˆ†")
            elif phoneme == 'r':
                suggestions.append("è‹±è¯­ 'r' éŸ³ï¼šèˆŒå°–ä¸è§¦ç¢°ä»»ä½•éƒ¨ä½ï¼Œè½»å¾®å·èµ·")
            elif phoneme == 'l':
                suggestions.append("'l' éŸ³ï¼šèˆŒå°–æŠµä½ä¸Šé½¿é¾ˆï¼Œæ°”æµä»èˆŒä¾§æµå‡º")
            elif phoneme in ['Ã¦', 'É‘Ë', 'É’']:
                suggestions.append("å¼€å£å…ƒéŸ³ï¼šéœ€è¦æ›´å¤§çš„å¼ å£åº¦å’Œæ›´ä½çš„èˆŒä½")
            elif phoneme in ['iË', 'Éª']:
                suggestions.append("é«˜å…ƒéŸ³ï¼šèˆŒä½è¦é«˜ï¼Œå˜´å‹ç•¥æ‰")
            elif phoneme in ['s', 'z']:
                suggestions.append("'s/z' éŸ³ï¼šèˆŒå°–æ¥è¿‘ä¸Šé½¿é¾ˆï¼Œä¿æŒç‹­çª„ç¼éš™")
            elif phoneme in ['Êƒ', 'Ê’']:
                suggestions.append("'sh/zh' éŸ³ï¼šèˆŒèº«æŠ¬é«˜ï¼Œå”‡éƒ¨ç•¥åœ†")
        
        return suggestions